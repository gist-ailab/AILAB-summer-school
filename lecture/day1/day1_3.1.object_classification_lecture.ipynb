{"cells":[{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:150%\">레이어</span>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","fc_layer = nn.Linear(7*7*32, 1)\n","\n","conv_layer = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n","\n","sequential_layer1 = nn.Sequential(\n","    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(fc_layer)\n","print(conv_layer)\n","print(sequential_layer1)"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:150%\">모델</span>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class BasicNet(nn.Module):               \n","  def __init__(self, output_shape=10):        \n","    super(BasicNet, self).__init__()        \n","    self.fc = nn.Linear(32*32*3, output_shape)   \n","  def forward(self, x):                \n","    out = self.fc(x)                \n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = BasicNet()\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class FCNet(nn.Module):                  \n","  def __init__(self, num_classes=10):         \n","    super(FCNet, self).__init__()          \n","    self.fc0 = nn.Linear(32*32*3, 256)       \n","    self.fc1 = nn.Linear(256, 128)         \n","    self.fc2 = nn.Linear(128, 64)          \n","    self.fc3 = nn.Linear(64, 32)          \n","    self.fc4 = nn.Linear(32, num_classes)\n","        \n","def forward(self, x):                 \n","    out0 = self.fc0(x)\n","    out1 = self.fc1(out0)\n","    out2 = self.fc2(out1)\n","    out3 = self.fc3(out2)\n","    out4 = self.fc4(out3)\n","    score = self.fc(out4) # Score\n","    # score = F.softmax(score)\n","    return score \n","  \n","model = FCNet()\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class ConvNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ConvNet, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        # Linear model (width*height*channel of the last feature map, Number of class)\n","        self.fc = nn.Linear(7*7*32, num_classes)\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        flatten = out.view(out.size(0), -1)  # Flatten\n","        score = self.fc(flatten) # Score\n","        # score = F.softmax(score) \n","        return score\n","    \n","model = ConvNet()\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision.models as models\n","import torchvision.models.detection as detection\n","\n","all_models = models.list_models()\n","classification_models = models.list_models(module=models)\n","detection_models = models.list_models(module=detection)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(classification_models)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(detection_models)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predefined Model\n","import torchvision.models as models\n","import torch.nn as nn\n","\n","model = models.resnet18(weights=None)\n","num_classes = 10\n","model.fc = nn.Linear(model.fc.in_features, num_classes)\n","\n","print(model)"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:150%\">손실 함수</span>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","criterion = nn.CrossEntropyLoss()\n","prediction = torch.tensor([1.2, 0.4, 0.2, 0.2])\n","groundTruth = torch.tensor([1.0, 0.0, 0.0, 0.0])\n","loss = criterion(prediction, groundTruth)\n","print(loss)"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:150%\">옵티마이저</span>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","model = nn.Linear(2,2, bias=False)\n","torch.nn.init.ones_(model.weight.data)\n","criterion = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# output1\n","optimizer.zero_grad()\n","input = torch.tensor([1.0,4.0])\n","output = model(input)\n","loss1 = criterion(input, output)\n","loss1.backward()\n","optimizer.step()\n","print(output.data)\n","print(loss1.data)\n","print(model.weight.data)\n","\n","#output2\n","optimizer.zero_grad()\n","output = model(input)\n","loss2 = criterion(input, output)\n","loss2.backward()\n","optimizer.step()\n","print(output.data)\n","print(loss2.data)\n","print(model.weight.data)"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:150%\">데이터셋</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13269,"status":"ok","timestamp":1684644697312,"user":{"displayName":"허윤재","userId":"12083611387958911250"},"user_tz":-540},"id":"_NSB6AqzpCAA","outputId":"05bfe48a-a90f-4f7f-eada-932965d24f08"},"outputs":[],"source":["from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","import os\n","data_root = os.path.join(os.path.dirname(os.getcwd()), 'data', 'cifar10')\n","os.makedirs(data_root, exist_ok=True)\n","\n","train_data = datasets.CIFAR10(\n","    root=data_root, # 불러올 데이터가 있는 path\n","    train=True, # train data를 불러올거면 True, test data를 불러올거면 False\n","    download=True, # 데이터가 존재하지 않으면 root에 해당 데이터셋을 다운로드\n","    transform=ToTensor() # dataset에 적용할 transform 지정\n",")\n","test_data = datasets.CIFAR10(\n","    root=data_root,\n","    train=False,\n","    download=False,\n","    transform=ToTensor()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1684644710514,"user":{"displayName":"허윤재","userId":"12083611387958911250"},"user_tz":-540},"id":"gYm_r2_Wpgth","outputId":"b37388ed-24a3-417e-9e70-a6ebe3d36938"},"outputs":[],"source":["# dimension order in torch : (channel, height, width)\n","print(train_data[0][0].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":1663,"status":"ok","timestamp":1684645000518,"user":{"displayName":"허윤재","userId":"12083611387958911250"},"user_tz":-540},"id":"eXZukHJuqFNJ","outputId":"4b53a885-d68d-4e9a-906e-bfd5590b539a"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","image = train_data[0][0]\n","plt.imshow(image.permute(1,2,0)) # order in plt should be (height, width, channel)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","from torch.utils.data import Dataset\n","import PIL.Image as Image\n","import pickle\n","\n","class CIFAR10(Dataset):\n","    base_folder = \"cifar-10-batches-py\"\n","    train_list = [\n","        [\"data_batch_1\", \"c99cafc152244af753f735de768cd75f\"],\n","        [\"data_batch_2\", \"d4bba439e000b95fd0a9bffe97cbabec\"],\n","        [\"data_batch_3\", \"54ebc095f3ab1f0389bbae665268c751\"],\n","        [\"data_batch_4\", \"634d18415352ddfa80567beed471001a\"],\n","        [\"data_batch_5\", \"482c414d41f54cd18b22e5b47cb7c3cb\"],\n","    ]\n","    test_list = [\n","        [\"test_batch\", \"40351d587109b95175f43aff81a1287e\"],\n","    ]\n","    meta = {\n","        \"filename\": \"batches.meta\",\n","        \"key\": \"label_names\",\n","        \"md5\": \"5ff9c542aee3614f3951f8cda6e48888\",\n","    }\n","\n","    def __init__(self, root, train=True, transform=None,target_transform=None):\n","        super(CIFAR10, self).__init__()\n","        self.root = root\n","        self.train = train\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        \n","        if self.train:\n","            downloaded_list = self.train_list\n","        else:\n","            downloaded_list = self.test_list\n","\n","        self.data = []\n","        self.targets = []\n","\n","        for file_name, _ in downloaded_list:\n","            file_path = os.path.join(self.root, self.base_folder, file_name)\n","            with open(file_path, \"rb\") as f:\n","                entry = pickle.load(f, encoding=\"latin1\")\n","                self.data.append(entry[\"data\"])\n","                if \"labels\" in entry:\n","                    self.targets.extend(entry[\"labels\"])\n","                else:\n","                    self.targets.extend(entry[\"fine_labels\"])\n","\n","        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n","        self.data = self.data.transpose((0, 2, 3, 1))\n","        self._load_meta()\n","\n","    def _load_meta(self):\n","        path = os.path.join(self.root, self.base_folder, self.meta[\"filename\"])\n","        with open(path, \"rb\") as infile:\n","            data = pickle.load(infile, encoding=\"latin1\")\n","            self.classes = data[self.meta[\"key\"]]\n","        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        img, target = self.data[index], self.targets[index]\n","        img = Image.fromarray(img)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","        return img, target"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","\n","data_root = os.path.join(os.path.dirname(os.getcwd()), 'data', 'cifar10')\n","\n","train_data = CIFAR10(\n","    root= data_root,\n","    train=True,\n","    transform=ToTensor(),\n",")\n","print(len(train_data))\n","\n","image = train_data[3][0]\n","plt.imshow(image.permute(1,2,0)) # order in plt should be (height, width, channel)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:150%\">어그멘테이션</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":681,"status":"ok","timestamp":1684645200361,"user":{"displayName":"허윤재","userId":"12083611387958911250"},"user_tz":-540},"id":"oVqqmGFEqszh","outputId":"036a7d93-4591-433d-8d85-159ed74ca5d1"},"outputs":[],"source":["import torchvision.transforms as transforms\n","\n","# Augmentation들 선언\n","normalize =transforms.Normalize(\n","    mean=[0.485, 0.456, 0.406],\n","    std=[0.229, 0.224, 0.225],\n","    )\n","center_crop = transforms.CenterCrop(16)\n","horizontal_flip = transforms.RandomHorizontalFlip(p=1)\n","vertical_flip = transforms.RandomVerticalFlip(p=1)\n","random_roation = transforms.RandomRotation([-45, 45])\n","gray_scale = transforms.Grayscale()\n","\n","#이미지에 각각의 Augmentation을 적용\n","normalized_img = normalize(image)\n","center_crop_img = center_crop(image)\n","h_flip_img = horizontal_flip(image)\n","v_flip_img = vertical_flip(image)\n","rotate_img = random_roation(image)\n","gray_img = gray_scale(image)\n","\n","# 적용된 Augmentation 결과들을 확인\n","fig = plt.figure(figsize=(16,16))\n","\n","for i in range(1,12,2):\n","    plt.subplot(6,2,i)\n","    plt.imshow(image.permute(1,2,0))\n","    plt.title('original')\n","    plt.axis('off')\n","\n","plt.subplot(6,2,2)\n","plt.imshow(normalized_img.permute(1, 2, 0))\n","plt.title('normalize')\n","plt.axis('off')\n","\n","plt.subplot(6,2,4)\n","plt.imshow(center_crop_img.permute(1,2,0))\n","plt.title('center crop')\n","plt.axis('off')\n","\n","plt.subplot(6,2,6)\n","plt.imshow(h_flip_img.permute(1, 2, 0))\n","plt.title('horizontal flip')\n","plt.axis('off')\n","\n","plt.subplot(6,2,8)\n","plt.imshow(v_flip_img.permute(1, 2, 0))\n","plt.title('vertical flip')\n","plt.axis('off')\n","\n","plt.subplot(6,2,10)\n","plt.imshow(rotate_img.permute(1, 2, 0))\n","plt.title('rotation')\n","plt.axis('off')\n","\n","plt.subplot(6,2,12)\n","plt.imshow(gray_img.permute(1, 2, 0), cmap='gray')\n","plt.title('gray scale')\n","plt.axis('off')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:150%\">데이터로더</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1684644754832,"user":{"displayName":"허윤재","userId":"12083611387958911250"},"user_tz":-540},"id":"fCJtS5pVpk9I","outputId":"cc084632-81d9-45ba-954a-33f4e25b0fec"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","trainloader = DataLoader(train_data, batch_size=256, shuffle=True, num_workers=0)\n","testloader = DataLoader(test_data, batch_size=256, shuffle=False, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = next(iter(trainloader))\n","print(data[0].shape)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyODUwU+N1PKdelHWfSPv/bJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
